#!/usr/bin/env python3
"""
æµ‹è¯•è‡ªé€‚åº”é«˜æ–¯åˆ†è£‚åŠŸèƒ½ï¼ˆè®ºæ–‡å…¬å¼40ï¼‰
éªŒè¯åŸºäºHessianèŒƒæ•°çš„è‡ªé€‚åº”é˜ˆå€¼æ˜¯å¦æ­£ç¡®å·¥ä½œ
"""

import torch
import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from scene.gaussian_model import GaussianModel
from utils.graphics_utils import BasicPointCloud

def create_test_gaussians(num_points=100):
    """åˆ›å»ºæµ‹è¯•ç”¨çš„é«˜æ–¯æ¨¡å‹"""
    print(f"åˆ›å»º {num_points} ä¸ªæµ‹è¯•é«˜æ–¯...")
    
    # åˆ›å»ºæµ‹è¯•ç‚¹äº‘
    points = np.random.randn(num_points, 3).astype(np.float32)
    colors = np.random.rand(num_points, 3).astype(np.float32)
    normals = np.zeros((num_points, 3), dtype=np.float32)
    
    pcd = BasicPointCloud(points=points, colors=colors, normals=normals)
    
    # åˆ›å»ºé«˜æ–¯æ¨¡å‹
    gaussians = GaussianModel(sh_degree=3)
    gaussians.create_from_pcd(pcd, spatial_lr_scale=1.0)
    
    return gaussians

def test_hessian_computation():
    """æµ‹è¯•1: HessianèŒƒæ•°è®¡ç®—"""
    print("\n" + "="*60)
    print("æµ‹è¯•1: HessianèŒƒæ•°è¿‘ä¼¼è®¡ç®—")
    print("="*60)
    
    gaussians = create_test_gaussians(50)
    
    # æ¨¡æ‹Ÿè®­ç»ƒè®¾ç½®
    class TrainingArgs:
        percent_dense = 0.01
        position_lr_init = 0.00016
        position_lr_final = 0.0000016
        position_lr_delay_mult = 0.01
        position_lr_max_steps = 30000
        feature_lr = 0.0025
        opacity_lr = 0.05
        scaling_lr = 0.005
        rotation_lr = 0.001
    
    gaussians.training_setup(TrainingArgs())
    
    # æ¨¡æ‹Ÿå‡ æ¬¡æ¢¯åº¦æ›´æ–°
    print("\næ¨¡æ‹Ÿæ¢¯åº¦æ›´æ–°...")
    for i in range(5):
        # åˆ›å»ºæ¨¡æ‹Ÿçš„è§†å›¾ç©ºé—´ç‚¹å¼ é‡
        viewspace_points = torch.randn(50, 3, device="cuda", requires_grad=True)
        
        # æ¨¡æ‹Ÿæ¢¯åº¦
        fake_loss = viewspace_points.sum()
        fake_loss.backward()
        
        # æ›´æ–°ç»Ÿè®¡
        update_filter = torch.ones(50, dtype=torch.bool, device="cuda")
        gaussians.add_densification_stats(viewspace_points, update_filter)
        
        print(f"  è¿­ä»£ {i+1}: æ¢¯åº¦ç´¯ç§¯å®Œæˆ")
    
    # è®¡ç®—HessianèŒƒæ•°
    hessian_norm = gaussians.compute_hessian_norm_approx()
    
    print(f"\nâœ… HessianèŒƒæ•°ç»Ÿè®¡:")
    print(f"  å¹³å‡å€¼: {hessian_norm.mean().item():.6f}")
    print(f"  æœ€å°å€¼: {hessian_norm.min().item():.6f}")
    print(f"  æœ€å¤§å€¼: {hessian_norm.max().item():.6f}")
    print(f"  æ ‡å‡†å·®: {hessian_norm.std().item():.6f}")
    
    # éªŒè¯èŒƒæ•°è‡³å°‘ä¸º1
    assert hessian_norm.min() >= 1.0, "HessianèŒƒæ•°åº”è¯¥è‡³å°‘ä¸º1"
    print("\nâœ… æµ‹è¯•1é€šè¿‡: HessianèŒƒæ•°è®¡ç®—æ­£ç¡®")
    
    return gaussians

def test_adaptive_threshold():
    """æµ‹è¯•2: è‡ªé€‚åº”é˜ˆå€¼è®¡ç®—"""
    print("\n" + "="*60)
    print("æµ‹è¯•2: è‡ªé€‚åº”é˜ˆå€¼è®¡ç®—ï¼ˆè®ºæ–‡å…¬å¼40ï¼‰")
    print("="*60)
    
    gaussians = create_test_gaussians(100)
    
    class TrainingArgs:
        percent_dense = 0.01
        position_lr_init = 0.00016
        position_lr_final = 0.0000016
        position_lr_delay_mult = 0.01
        position_lr_max_steps = 30000
        feature_lr = 0.0025
        opacity_lr = 0.05
        scaling_lr = 0.005
        rotation_lr = 0.001
    
    gaussians.training_setup(TrainingArgs())
    
    # æ¨¡æ‹Ÿä¸åŒæ›²ç‡çš„åŒºåŸŸ
    print("\nåˆ›å»ºä¸åŒæ›²ç‡çš„æµ‹è¯•åœºæ™¯...")
    
    # åŒºåŸŸ1: é«˜æ›²ç‡ï¼ˆå¤§æ¢¯åº¦å˜åŒ–ï¼‰
    high_curvature_indices = torch.arange(0, 30, device="cuda")
    # åŒºåŸŸ2: ä½æ›²ç‡ï¼ˆå°æ¢¯åº¦å˜åŒ–ï¼‰
    low_curvature_indices = torch.arange(30, 60, device="cuda")
    # åŒºåŸŸ3: ä¸­ç­‰æ›²ç‡
    medium_curvature_indices = torch.arange(60, 100, device="cuda")
    
    # æ¨¡æ‹Ÿæ¢¯åº¦æ›´æ–°ï¼ˆä¸åŒåŒºåŸŸä¸åŒçš„æ¢¯åº¦å˜åŒ–ï¼‰
    for i in range(10):
        viewspace_points = torch.zeros(100, 3, device="cuda", requires_grad=True)
        
        # é«˜æ›²ç‡åŒºåŸŸï¼šå¤§æ¢¯åº¦å˜åŒ–
        viewspace_points.data[high_curvature_indices] = torch.randn(30, 3, device="cuda") * 2.0
        
        # ä½æ›²ç‡åŒºåŸŸï¼šå°æ¢¯åº¦å˜åŒ–
        viewspace_points.data[low_curvature_indices] = torch.randn(30, 3, device="cuda") * 0.1
        
        # ä¸­ç­‰æ›²ç‡åŒºåŸŸ
        viewspace_points.data[medium_curvature_indices] = torch.randn(40, 3, device="cuda") * 0.5
        
        fake_loss = viewspace_points.sum()
        fake_loss.backward()
        
        update_filter = torch.ones(100, dtype=torch.bool, device="cuda")
        gaussians.add_densification_stats(viewspace_points, update_filter)
    
    # è®¡ç®—HessianèŒƒæ•°
    hessian_norm = gaussians.compute_hessian_norm_approx()
    
    print(f"\nä¸åŒåŒºåŸŸçš„HessianèŒƒæ•°:")
    print(f"  é«˜æ›²ç‡åŒºåŸŸ: {hessian_norm[high_curvature_indices].mean().item():.6f}")
    print(f"  ä½æ›²ç‡åŒºåŸŸ: {hessian_norm[low_curvature_indices].mean().item():.6f}")
    print(f"  ä¸­ç­‰æ›²ç‡åŒºåŸŸ: {hessian_norm[medium_curvature_indices].mean().item():.6f}")
    
    # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
    base_threshold = 0.0002
    adaptive_threshold = base_threshold / hessian_norm.squeeze()
    
    print(f"\nè‡ªé€‚åº”é˜ˆå€¼ï¼ˆåŸºç¡€é˜ˆå€¼={base_threshold}ï¼‰:")
    print(f"  é«˜æ›²ç‡åŒºåŸŸ: {adaptive_threshold[high_curvature_indices].mean().item():.8f}")
    print(f"  ä½æ›²ç‡åŒºåŸŸ: {adaptive_threshold[low_curvature_indices].mean().item():.8f}")
    print(f"  ä¸­ç­‰æ›²ç‡åŒºåŸŸ: {adaptive_threshold[medium_curvature_indices].mean().item():.8f}")
    
    # éªŒè¯ï¼šé«˜æ›²ç‡åŒºåŸŸåº”è¯¥æœ‰æ›´ä½çš„é˜ˆå€¼ï¼ˆæ›´å®¹æ˜“åˆ†è£‚ï¼‰
    high_curve_threshold = adaptive_threshold[high_curvature_indices].mean()
    low_curve_threshold = adaptive_threshold[low_curvature_indices].mean()
    
    print(f"\nâœ… é˜ˆå€¼æ¯”è¾ƒ:")
    print(f"  é«˜æ›²ç‡é˜ˆå€¼ < ä½æ›²ç‡é˜ˆå€¼: {high_curve_threshold < low_curve_threshold}")
    
    assert high_curve_threshold < low_curve_threshold, \
        "é«˜æ›²ç‡åŒºåŸŸåº”è¯¥æœ‰æ›´ä½çš„é˜ˆå€¼ï¼ˆæ›´å®¹æ˜“åˆ†è£‚ï¼‰"
    
    print("\nâœ… æµ‹è¯•2é€šè¿‡: è‡ªé€‚åº”é˜ˆå€¼è®¡ç®—æ­£ç¡®")
    
    return gaussians

def test_split_comparison():
    """æµ‹è¯•3: å¯¹æ¯”å›ºå®šé˜ˆå€¼ vs è‡ªé€‚åº”é˜ˆå€¼"""
    print("\n" + "="*60)
    print("æµ‹è¯•3: å›ºå®šé˜ˆå€¼ vs è‡ªé€‚åº”é˜ˆå€¼åˆ†è£‚å¯¹æ¯”")
    print("="*60)
    
    # åˆ›å»ºä¸¤ä¸ªç›¸åŒçš„é«˜æ–¯æ¨¡å‹
    gaussians_fixed = create_test_gaussians(50)
    gaussians_adaptive = create_test_gaussians(50)
    
    class TrainingArgs:
        percent_dense = 0.01
        position_lr_init = 0.00016
        position_lr_final = 0.0000016
        position_lr_delay_mult = 0.01
        position_lr_max_steps = 30000
        feature_lr = 0.0025
        opacity_lr = 0.05
        scaling_lr = 0.005
        rotation_lr = 0.001
    
    gaussians_fixed.training_setup(TrainingArgs())
    gaussians_adaptive.training_setup(TrainingArgs())
    
    # æ¨¡æ‹Ÿæ¢¯åº¦æ›´æ–°
    print("\næ¨¡æ‹Ÿæ¢¯åº¦æ›´æ–°...")
    for i in range(10):
        viewspace_points = torch.randn(50, 3, device="cuda", requires_grad=True)
        fake_loss = viewspace_points.sum()
        fake_loss.backward()
        
        update_filter = torch.ones(50, dtype=torch.bool, device="cuda")
        gaussians_fixed.add_densification_stats(viewspace_points, update_filter)
        gaussians_adaptive.add_densification_stats(viewspace_points, update_filter)
    
    # è®¡ç®—å¹³å‡æ¢¯åº¦
    grads_fixed = gaussians_fixed.xyz_gradient_accum / gaussians_fixed.denom
    grads_adaptive = gaussians_adaptive.xyz_gradient_accum / gaussians_adaptive.denom
    
    # è®°å½•åˆå§‹ç‚¹æ•°
    initial_points = gaussians_fixed.get_xyz.shape[0]
    print(f"\nåˆå§‹é«˜æ–¯æ•°é‡: {initial_points}")
    
    # ä½¿ç”¨å›ºå®šé˜ˆå€¼åˆ†è£‚
    print("\nä½¿ç”¨å›ºå®šé˜ˆå€¼åˆ†è£‚...")
    gaussians_fixed.densify_and_split(
        grads_fixed, 
        grad_threshold=0.0002, 
        scene_extent=1.0,
        use_adaptive_threshold=False
    )
    points_after_fixed = gaussians_fixed.get_xyz.shape[0]
    print(f"  å›ºå®šé˜ˆå€¼å: {points_after_fixed} ä¸ªé«˜æ–¯")
    
    # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼åˆ†è£‚
    print("\nä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼åˆ†è£‚...")
    gaussians_adaptive.densify_and_split(
        grads_adaptive, 
        grad_threshold=0.0002, 
        scene_extent=1.0,
        use_adaptive_threshold=True
    )
    points_after_adaptive = gaussians_adaptive.get_xyz.shape[0]
    print(f"  è‡ªé€‚åº”é˜ˆå€¼å: {points_after_adaptive} ä¸ªé«˜æ–¯")
    
    print(f"\nâœ… åˆ†è£‚ç»“æœå¯¹æ¯”:")
    print(f"  å›ºå®šé˜ˆå€¼å¢åŠ : {points_after_fixed - initial_points} ä¸ªé«˜æ–¯")
    print(f"  è‡ªé€‚åº”é˜ˆå€¼å¢åŠ : {points_after_adaptive - initial_points} ä¸ªé«˜æ–¯")
    print(f"  å·®å¼‚: {abs(points_after_adaptive - points_after_fixed)} ä¸ªé«˜æ–¯")
    
    print("\nâœ… æµ‹è¯•3é€šè¿‡: åˆ†è£‚åŠŸèƒ½æ­£å¸¸å·¥ä½œ")

def test_integration():
    """æµ‹è¯•4: å®Œæ•´é›†æˆæµ‹è¯•"""
    print("\n" + "="*60)
    print("æµ‹è¯•4: å®Œæ•´é›†æˆæµ‹è¯•")
    print("="*60)
    
    gaussians = create_test_gaussians(100)
    
    class TrainingArgs:
        percent_dense = 0.01
        position_lr_init = 0.00016
        position_lr_final = 0.0000016
        position_lr_delay_mult = 0.01
        position_lr_max_steps = 30000
        feature_lr = 0.0025
        opacity_lr = 0.05
        scaling_lr = 0.005
        rotation_lr = 0.001
    
    gaussians.training_setup(TrainingArgs())
    
    print("\næ¨¡æ‹Ÿå®Œæ•´çš„å¯†é›†åŒ–æµç¨‹...")
    initial_points = gaussians.get_xyz.shape[0]
    
    # æ¨¡æ‹Ÿå¤šæ¬¡è¿­ä»£
    for iteration in range(20):
        # æ¨¡æ‹Ÿæ¸²æŸ“å’Œæ¢¯åº¦è®¡ç®—
        viewspace_points = torch.randn(gaussians.get_xyz.shape[0], 3, device="cuda", requires_grad=True)
        fake_loss = viewspace_points.sum()
        fake_loss.backward()
        
        # æ›´æ–°ç»Ÿè®¡
        update_filter = torch.ones(gaussians.get_xyz.shape[0], dtype=torch.bool, device="cuda")
        gaussians.add_densification_stats(viewspace_points, update_filter)
        
        # æ¯5æ¬¡è¿­ä»£æ‰§è¡Œä¸€æ¬¡å¯†é›†åŒ–
        if (iteration + 1) % 5 == 0:
            grads = gaussians.xyz_gradient_accum / gaussians.denom
            gaussians.densify_and_prune(
                max_grad=0.0002,
                min_opacity=0.005,
                extent=1.0,
                max_screen_size=20
            )
            print(f"  è¿­ä»£ {iteration+1}: å½“å‰é«˜æ–¯æ•°é‡ = {gaussians.get_xyz.shape[0]}")
    
    final_points = gaussians.get_xyz.shape[0]
    
    print(f"\nâœ… å®Œæ•´æµç¨‹ç»“æœ:")
    print(f"  åˆå§‹é«˜æ–¯: {initial_points}")
    print(f"  æœ€ç»ˆé«˜æ–¯: {final_points}")
    print(f"  å‡€å¢åŠ : {final_points - initial_points}")
    
    print("\nâœ… æµ‹è¯•4é€šè¿‡: å®Œæ•´é›†æˆæµ‹è¯•æˆåŠŸ")

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("="*60)
    print("è‡ªé€‚åº”é«˜æ–¯åˆ†è£‚åŠŸèƒ½æµ‹è¯•ï¼ˆè®ºæ–‡å…¬å¼40ï¼‰")
    print("="*60)
    
    try:
        # æµ‹è¯•1: HessianèŒƒæ•°è®¡ç®—
        test_hessian_computation()
        
        # æµ‹è¯•2: è‡ªé€‚åº”é˜ˆå€¼
        test_adaptive_threshold()
        
        # æµ‹è¯•3: åˆ†è£‚å¯¹æ¯”
        test_split_comparison()
        
        # æµ‹è¯•4: å®Œæ•´é›†æˆ
        test_integration()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("="*60)
        print("\nâœ… è‡ªé€‚åº”é«˜æ–¯åˆ†è£‚åŠŸèƒ½å·²å®Œæ•´å®ç°ï¼ˆ100%ï¼‰")
        print("âœ… è®ºæ–‡å…¬å¼40: åˆ†è£‚é˜ˆå€¼ âˆ 1 / max(1, ||H(Î¼_c)||_F)")
        print("âœ… é«˜æ›²ç‡åŒºåŸŸè‡ªåŠ¨ä½¿ç”¨æ›´ä½é˜ˆå€¼ï¼Œå®ç°æ›´ç»†ç²’åº¦åˆ†è£‚")
        print("âœ… ä½æ›²ç‡åŒºåŸŸä¿æŒè¾ƒé«˜é˜ˆå€¼ï¼Œé¿å…è¿‡åº¦åˆ†è£‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
